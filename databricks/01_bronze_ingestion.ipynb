{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "id": "cell-bronze-header",
            "metadata": {},
            "source": [
                "# ðŸ¥‰ Bronze Layer â€” Raw Ingestion\n",
                "\n",
                "**Medallion Architecture: Bronze â†’ Silver â†’ Gold**\n",
                "\n",
                "This notebook ingests the 6 raw CSV files provided for the Annie's Magic Numbers\n",
                "challenge into Delta tables with zero business transformation.\n",
                "\n",
                "| Source File | Bronze Table |\n",
                "|---|---|\n",
                "| BegInvFINAL12312016.csv | bronze.beg_inventory |\n",
                "| EndInvFINAL12312016.csv | bronze.end_inventory |\n",
                "| PurchasesFINAL12312016.csv | bronze.purchases |\n",
                "| SalesFINAL12312016.csv | bronze.sales |\n",
                "| InvoicePurchases12312016.csv | bronze.invoice_purchases |\n",
                "| 2017PurchasePricesDec.csv | bronze.purchase_prices |\n",
                "\n",
                "---\n",
                "**Author:** Data Engineering Team  \n",
                "**Last Updated:** 2026-02-24  \n",
                "**Run Order:** 1 of 3"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cell-bronze-config-header",
            "metadata": {},
            "source": [
                "## 0. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell-bronze-config",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import functions as F\n",
                "from pyspark.sql.types import StructType\n",
                "from datetime import datetime\n",
                "\n",
                "# ---------------------------------------------------------------------------\n",
                "# CONFIG â€” Update DBFS_ROOT to wherever you uploaded the files in your workspace\n",
                "# Recommended: /Volumes/<catalog>/<schema>/raw_files/  (Unity Catalog Volume)\n",
                "#              or /FileStore/annie_magic_numbers/ (legacy DBFS)\n",
                "# ---------------------------------------------------------------------------\n",
                "DBFS_ROOT    = \"/FileStore/annie_magic_numbers\"   # â† UPDATE THIS PATH\n",
                "BRONZE_DB    = \"bronze\"\n",
                "CATALOG      = \"main\"                             # â† UPDATE if using Unity Catalog\n",
                "\n",
                "# Fully-qualified database name\n",
                "BRONZE_SCHEMA = f\"{CATALOG}.{BRONZE_DB}\"\n",
                "\n",
                "# Ingestion timestamp (consistent across all tables in a single run)\n",
                "RUN_TS = datetime.utcnow().isoformat()\n",
                "\n",
                "print(f\"Source path  : {DBFS_ROOT}\")\n",
                "print(f\"Target schema: {BRONZE_SCHEMA}\")\n",
                "print(f\"Run timestamp: {RUN_TS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cell-bronze-create-db-header",
            "metadata": {},
            "source": [
                "## 1. Create Bronze Database / Schema"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell-bronze-create-db",
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {BRONZE_SCHEMA}\")\n",
                "print(f\"âœ…  Schema '{BRONZE_SCHEMA}' is ready.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cell-bronze-helper-header",
            "metadata": {},
            "source": [
                "## 2. Helper â€” Generic CSV â†’ Delta Ingestion"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell-bronze-helper",
            "metadata": {},
            "outputs": [],
            "source": [
                "def ingest_csv_to_bronze(\n",
                "    source_path: str,\n",
                "    target_table: str,\n",
                "    *,\n",
                "    header: bool = True,\n",
                "    infer_schema: bool = True,\n",
                "    multiline: bool = False\n",
                ") -> None:\n",
                "    \"\"\"\n",
                "    Read a CSV from DBFS and write it as a Delta table in the Bronze schema.\n",
                "\n",
                "    Args:\n",
                "        source_path  : Full DBFS path to the CSV file.\n",
                "        target_table : Fully-qualified target Delta table (schema.table).\n",
                "        header       : Whether the CSV has a header row.\n",
                "        infer_schema : If True, Spark infers column types; otherwise all strings.\n",
                "        multiline    : Set True for CSVs that may contain embedded newlines.\n",
                "    \"\"\"\n",
                "    print(f\"\\nðŸ“¥  Ingesting  {source_path}\")\n",
                "    print(f\"       â†’ {target_table}\")\n",
                "\n",
                "    df = (\n",
                "        spark.read\n",
                "             .option(\"header\", str(header).lower())\n",
                "             .option(\"inferSchema\", str(infer_schema).lower())\n",
                "             .option(\"multiLine\", str(multiline).lower())\n",
                "             .option(\"escape\", '\"')\n",
                "             .option(\"quote\", '\"')\n",
                "             .option(\"encoding\", \"UTF-8\")\n",
                "             .csv(source_path)\n",
                "    )\n",
                "\n",
                "    # Add metadata columns required by the Medallion pattern\n",
                "    df = df.withColumn(\"_ingestion_timestamp\", F.lit(RUN_TS)) \\\n",
                "           .withColumn(\"_source_file\", F.lit(source_path))\n",
                "\n",
                "    row_count = df.count()\n",
                "    print(f\"       rows read : {row_count:,}\")\n",
                "\n",
                "    # Write as Delta â€” overwrite ensures idempotency on re-runs\n",
                "    (\n",
                "        df.write\n",
                "          .format(\"delta\")\n",
                "          .mode(\"overwrite\")\n",
                "          .option(\"overwriteSchema\", \"true\")\n",
                "          .saveAsTable(target_table)\n",
                "    )\n",
                "\n",
                "    print(f\"   âœ…  Written to {target_table}  ({row_count:,} rows)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cell-bronze-ingest-header",
            "metadata": {},
            "source": [
                "## 3. Ingest Each Source File"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell-bronze-ingest-beg",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.1 â€” Beginning Inventory (Jan 1, 2016)\n",
                "ingest_csv_to_bronze(\n",
                "    source_path  = f\"{DBFS_ROOT}/BegInvFINAL12312016.csv\",\n",
                "    target_table = f\"{BRONZE_SCHEMA}.beg_inventory\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell-bronze-ingest-end",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.2 â€” Ending Inventory (Dec 31, 2016)\n",
                "ingest_csv_to_bronze(\n",
                "    source_path  = f\"{DBFS_ROOT}/EndInvFINAL12312016.csv\",\n",
                "    target_table = f\"{BRONZE_SCHEMA}.end_inventory\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell-bronze-ingest-purch",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.3 â€” All Purchase Transactions 2016 (large file ~400 MB)\n",
                "ingest_csv_to_bronze(\n",
                "    source_path  = f\"{DBFS_ROOT}/PurchasesFINAL12312016.csv\",\n",
                "    target_table = f\"{BRONZE_SCHEMA}.purchases\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell-bronze-ingest-sales",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.4 â€” All Sales Transactions 2016 (largest file ~1.7 GB)\n",
                "ingest_csv_to_bronze(\n",
                "    source_path  = f\"{DBFS_ROOT}/SalesFINAL12312016.csv\",\n",
                "    target_table = f\"{BRONZE_SCHEMA}.sales\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell-bronze-ingest-invoice",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.5 â€” Invoice-level Purchase Summaries\n",
                "ingest_csv_to_bronze(\n",
                "    source_path  = f\"{DBFS_ROOT}/InvoicePurchases12312016.csv\",\n",
                "    target_table = f\"{BRONZE_SCHEMA}.invoice_purchases\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell-bronze-ingest-prices",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.6 â€” 2017 Reference Purchase Prices (cost basis)\n",
                "ingest_csv_to_bronze(\n",
                "    source_path  = f\"{DBFS_ROOT}/2017PurchasePricesDec.csv\",\n",
                "    target_table = f\"{BRONZE_SCHEMA}.purchase_prices\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cell-bronze-validation-header",
            "metadata": {},
            "source": [
                "## 4. Validation Checks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell-bronze-validation",
            "metadata": {},
            "outputs": [],
            "source": [
                "tables = [\n",
                "    \"beg_inventory\",\n",
                "    \"end_inventory\",\n",
                "    \"purchases\",\n",
                "    \"sales\",\n",
                "    \"invoice_purchases\",\n",
                "    \"purchase_prices\",\n",
                "]\n",
                "\n",
                "print(\"\\nðŸ“Š  Bronze Layer Validation Report\")\n",
                "print(\"=\" * 55)\n",
                "\n",
                "for tbl in tables:\n",
                "    full_name = f\"{BRONZE_SCHEMA}.{tbl}\"\n",
                "    count = spark.table(full_name).count()\n",
                "    col_count = len(spark.table(full_name).columns)\n",
                "    print(f\"  {tbl:<25} | {count:>12,} rows | {col_count} cols\")\n",
                "\n",
                "print(\"=\" * 55)\n",
                "print(\"âœ…  All Bronze tables successfully ingested.\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cell-bronze-schema-header",
            "metadata": {},
            "source": [
                "## 5. Schema Preview\n",
                "\n",
                "Quick visual confirmation of the Bronze schemas."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell-bronze-schema",
            "metadata": {},
            "outputs": [],
            "source": [
                "for tbl in tables:\n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"  {BRONZE_SCHEMA}.{tbl}\")\n",
                "    print(f\"{'='*50}\")\n",
                "    spark.table(f\"{BRONZE_SCHEMA}.{tbl}\").printSchema()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cell-bronze-complete",
            "metadata": {},
            "source": [
                "## âœ… Bronze Ingestion Complete\n",
                "\n",
                "**Next step:** Run `02_silver_cleaning.ipynb` to clean, cast types, and enrich the data."
            ]
        }
    ]
}