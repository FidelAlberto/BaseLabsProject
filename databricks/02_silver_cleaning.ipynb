{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \ud83e\udd48 Silver Layer \u2014 Cleaning & Enrichment\n",
                "\n",
                "**Annie's Magic Numbers Medallion Architecture**\n",
                "\n",
                "This notebook reads Bronze Delta tables, cleans the data, and writes to the Silver layer."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83d\udd10 Configuration \u2014 ADLS Gen2 Authentication"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CELL 0 \u2014 Azure Data Lake Gen2 Authentication\n",
                "# ============================================================\n",
                "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
                "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
                "spark.conf.set(\"spark.databricks.delta.autoOptimize.optimizeWrite\", \"true\")\n",
                "\n",
                "spark.conf.set(\n",
                "    \"fs.azure.account.key.anniedatalake123.dfs.core.windows.net\",\n",
                "    \"<REDACTED_AZURE_STORAGE_KEY>\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83d\udfe6 Path Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import functions as F\n",
                "from pyspark.sql.types import DoubleType, IntegerType, LongType, StringType, DateType\n",
                "\n",
                "container_name = \"annie-data\"\n",
                "storage_account = \"anniedatalake123\"\n",
                "\n",
                "base_path = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/\"\n",
                "bronze_path = base_path + \"bronze/\"\n",
                "silver_path = base_path + \"silver/\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83d\udfe6 Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def normalize_columns(df):\n",
                "    import re\n",
                "    def to_snake(name):\n",
                "        s1 = re.sub(r'(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
                "        return re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower().strip()\n",
                "\n",
                "    new_cols = {c: to_snake(c) for c in df.columns if not c.startswith(\"_\")}\n",
                "    for old, new in new_cols.items():\n",
                "        if old != new:\n",
                "            df = df.withColumnRenamed(old, new)\n",
                "    return df\n",
                "\n",
                "def write_silver(df, table_name):\n",
                "    # Optimizamos la escritura para Delta Lake sin particionamiento f\u00edsico lento\n",
                "    target_path = silver_path + table_name\n",
                "    (\n",
                "        df.write\n",
                "          .format(\"delta\")\n",
                "          .mode(\"overwrite\")\n",
                "          .option(\"overwriteSchema\", \"true\")\n",
                "          .save(target_path)\n",
                "    )\n",
                "    # Feedback instant\u00e1neo usando el cache\n",
                "    count = df.count()\n",
                "    print(f\"   \u2705 silver.{table_name} guardada exitosamente \u2192 {count:,} filas\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83e\udd48 Silver \u2014 Beginning Inventory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Processing silver.beg_inventory ...\")\n",
                "beg_inv_raw = spark.read.format(\"delta\").load(bronze_path + \"begin_inventory\")\n",
                "beg_inv = normalize_columns(beg_inv_raw)\n",
                "\n",
                "beg_inv = (\n",
                "    beg_inv\n",
                "    .withColumn(\"on_hand\",    F.col(\"on_hand\").cast(IntegerType()))\n",
                "    .withColumn(\"price\",      F.col(\"price\").cast(DoubleType()))\n",
                "    .withColumn(\"total_cost\", F.round(F.col(\"on_hand\") * F.col(\"price\"), 2))\n",
                "    .withColumn(\"start_date\", F.to_date(F.col(\"start_date\"), \"yyyy-MM-dd\"))\n",
                "    .filter(F.col(\"inventory_id\").isNotNull())\n",
                "    .filter(F.col(\"brand\").isNotNull())\n",
                "    .filter(F.col(\"price\") > 0)\n",
                "    .dropDuplicates([\"inventory_id\"])\n",
                "    .withColumn(\"inventory_value\", F.col(\"total_cost\"))\n",
                ")\n",
                "\n",
                "\n",
                "beg_inv = beg_inv.repartition(4).cache()\n",
                "beg_inv.count()\n",
                "write_silver(beg_inv, \"beg_inventory\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83e\udd48 Silver \u2014 Ending Inventory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Processing silver.end_inventory ...\")\n",
                "end_inv_raw = spark.read.format(\"delta\").load(bronze_path + \"end_inventory\")\n",
                "end_inv = normalize_columns(end_inv_raw)\n",
                "\n",
                "end_inv = (\n",
                "    end_inv\n",
                "    .withColumn(\"on_hand\",    F.col(\"on_hand\").cast(IntegerType()))\n",
                "    .withColumn(\"price\",      F.col(\"price\").cast(DoubleType()))\n",
                "    .withColumn(\"total_cost\", F.round(F.col(\"on_hand\") * F.col(\"price\"), 2))\n",
                "    .withColumn(\"end_date\",   F.to_date(F.col(\"end_date\"), \"yyyy-MM-dd\"))\n",
                "    .filter(F.col(\"inventory_id\").isNotNull())\n",
                "    .filter(F.col(\"brand\").isNotNull())\n",
                "    .filter(F.col(\"price\") > 0)\n",
                "    .dropDuplicates([\"inventory_id\"])\n",
                "    .withColumn(\"inventory_value\", F.col(\"total_cost\"))\n",
                ")\n",
                "\n",
                "\n",
                "end_inv = end_inv.repartition(4).cache()\n",
                "end_inv.count()\n",
                "write_silver(end_inv, \"end_inventory\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83e\udd48 Silver \u2014 Purchase Prices"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Processing silver.purchase_prices ...\")\n",
                "pp_raw = spark.read.format(\"delta\").load(bronze_path + \"prices\")\n",
                "pp = normalize_columns(pp_raw)\n",
                "\n",
                "pp = (\n",
                "    pp\n",
                "    .withColumn(\"price\", F.col(\"price\").cast(DoubleType()))\n",
                "    .filter(F.col(\"brand\").isNotNull())\n",
                "    .filter(F.col(\"description\").isNotNull())\n",
                "    .filter(F.col(\"price\") > 0)\n",
                "    .withColumn(\"brand\", F.col(\"brand\").cast(IntegerType()))\n",
                "    .dropDuplicates([\"brand\", \"description\"])\n",
                ")\n",
                "\n",
                "\n",
                "pp = pp.repartition(4).cache()\n",
                "pp.count()\n",
                "write_silver(pp, \"purchase_prices\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83e\udd48 Silver \u2014 Invoice Purchases"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Processing silver.invoice_purchases ...\")\n",
                "inv_raw = spark.read.format(\"delta\").load(bronze_path + \"invoices\")\n",
                "inv = normalize_columns(inv_raw)\n",
                "\n",
                "inv = (\n",
                "    inv\n",
                "    .withColumn(\"vendor_number\",  F.col(\"vendor_number\").cast(IntegerType()))\n",
                "    .withColumn(\"quantity\",       F.col(\"quantity\").cast(IntegerType()))\n",
                "    .withColumn(\"dollars\",        F.col(\"dollars\").cast(DoubleType()))\n",
                "    .withColumn(\"freight\",        F.col(\"freight\").cast(DoubleType()))\n",
                "    .withColumn(\"invoice_date\",   F.to_date(F.col(\"invoice_date\"), \"MM/dd/yyyy\"))\n",
                "    .withColumn(\"pay_date\",       F.to_date(F.col(\"pay_date\"), \"MM/dd/yyyy\"))\n",
                "    .filter(F.col(\"po_number\").isNotNull())\n",
                "    .dropDuplicates([\"vendor_number\", \"po_number\", \"invoice_date\"])\n",
                ")\n",
                "\n",
                "\n",
                "inv = inv.repartition(4).cache()\n",
                "inv.count()\n",
                "write_silver(inv, \"invoice_purchases\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83e\udd48 Silver \u2014 Purchases (Enriched)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Processing silver.purchases ...\")\n",
                "purch_raw = spark.read.format(\"delta\").load(bronze_path + \"purchases\")\n",
                "purch = normalize_columns(purch_raw)\n",
                "\n",
                "purch = (\n",
                "    purch\n",
                "    .withColumn(\"vendor_number\",   F.col(\"vendor_number\").cast(IntegerType()))\n",
                "    .withColumn(\"quantity\",        F.col(\"quantity\").cast(IntegerType()))\n",
                "    .withColumn(\"dollars\",         F.col(\"dollars\").cast(DoubleType()))\n",
                "    .withColumn(\"purchase_price\",  F.col(\"purchase_price\").cast(DoubleType()))\n",
                "    .withColumn(\"brand\",           F.col(\"brand\").cast(IntegerType()))\n",
                "    .withColumn(\"po_date\",         F.to_date(F.col(\"po_date\"), \"MM/dd/yyyy\"))\n",
                "    .withColumn(\"receiving_date\",  F.to_date(F.col(\"receiving_date\"), \"MM/dd/yyyy\"))\n",
                "    .withColumn(\"invoice_date\",    F.to_date(F.col(\"invoice_date\"), \"MM/dd/yyyy\"))\n",
                "    .filter(F.col(\"brand\").isNotNull())\n",
                "    .filter(F.col(\"quantity\") > 0)\n",
                "    .dropDuplicates([\"vendor_number\", \"po_number\", \"brand\", \"description\", \"receiving_date\"])\n",
                ")\n",
                "\n",
                "# OPTIMIZACI\u00d3N: Usar 'pp' de memoria (ya cacheado) en lugar de leer disk\n",
                "pp_lookup = pp.select(\n",
                "    F.col(\"brand\").alias(\"ref_brand\"),\n",
                "    F.col(\"description\").alias(\"ref_description\"),\n",
                "    F.col(\"price\").alias(\"ref_price\")\n",
                ")\n",
                "\n",
                "purch_enriched = (\n",
                "    purch.join(\n",
                "        F.broadcast(pp_lookup),\n",
                "        on=[purch[\"brand\"] == pp_lookup[\"ref_brand\"],\n",
                "            purch[\"description\"] == pp_lookup[\"ref_description\"]],\n",
                "        how=\"left\"\n",
                "    )\n",
                "    .withColumn(\"cost_per_unit\", F.round(F.coalesce(F.col(\"ref_price\"), F.col(\"purchase_price\")), 4))\n",
                "    .drop(\"ref_brand\", \"ref_description\", \"ref_price\")\n",
                "    .withColumn(\"total_cost\", F.round(F.col(\"cost_per_unit\") * F.col(\"quantity\"), 2))\n",
                ")\n",
                "\n",
                "purch_enriched = purch_enriched.repartition(8).cache()\n",
                "purch_enriched.count()\n",
                "write_silver(purch_enriched, \"purchases\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83e\udd48 Silver \u2014 Sales"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Sales...')\n",
                "s_raw = spark.read.format('delta').load(bronze_path + 'sales')\n",
                "sales = normalize_columns(s_raw).repartition(8).cache()\n",
                "sales.count()\n",
                "write_silver(sales, 'sales')\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
