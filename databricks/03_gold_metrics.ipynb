{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#  Gold Layer — Business Metrics\n",
                "\n",
                "**Annie's Magic Numbers — Medallion Architecture**\n",
                "\n",
                "Produces the 10 analytical tables for Power BI using clean data from Silver.\n",
                "All tables are **aggregated** and contain **only the columns used in the dashboard**.\n",
                "\n",
                "### Gold Tables (columns = only those consumed by Power BI):\n",
                "| # | Table | Columns |\n",
                "|---|---|---|\n",
                "| 1 | `gold.sales_enriched` | brand, description, size, classification, store, sales_quantity, sales_dollars, profit_dollars, volume |\n",
                "| 2 | `gold.product_profitability` | brand, description, size, classification, total_units_sold, total_revenue, total_profit_dollars, avg_margin_pct, is_loss_maker, rank_by_profit, rank_by_margin |\n",
                "| 3 | `gold.brand_profitability` | brand, classification, sku_count, total_units_sold, total_revenue, total_profit_dollars, avg_margin_pct, is_loss_maker, rank_by_profit, rank_by_margin |\n",
                "| 4 | `gold.loss_makers` | level, brand, description, size, classification, stores_stocking, total_units_sold, total_revenue, total_profit_dollars, avg_margin_pct, recommendation |\n",
                "| 5 | `gold.sales_by_store` | store, transaction_count, unique_brands, unique_skus, total_units_sold, total_revenue, total_profit_dollars, avg_margin_pct |\n",
                "| 6 | `gold.sales_time_series` | sale_year, sale_month, sale_month_name, monthly_revenue, monthly_profit, avg_margin_pct, total_units_sold, active_brands, active_stores, transaction_count |\n",
                "| 7 | `gold.inventory_delta` | brand, description, size, vendor_name, beg_on_hand, end_on_hand, inventory_change, inventory_value_change, stock_status |\n",
                "| 8 | `gold.vendor_performance` | vendor_name, brands_supplied, skus_supplied, total_purchase_spend, avg_cost_per_unit, avg_lead_time_days, total_po_count |\n",
                "| 9 | `gold.size_analysis` | size, unique_skus, unique_brands, total_units_sold, total_profit_dollars, avg_margin_pct, avg_selling_price |\n",
                "| 10 | `gold.classification_performance` | classification, unique_brands, unique_skus, total_units_sold, total_revenue, total_profit_dollars, overall_margin_pct |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "###  Configuration — ADLS Gen2 Authentication"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
                "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
                "spark.conf.set(\"spark.databricks.delta.autoOptimize.optimizeWrite\", \"true\")\n",
                "\n",
                "spark.conf.set(\n",
                "    \"fs.azure.account.key.anniedatalake123.dfs.core.windows.net\",\n",
                "    \"<REDACTED_AZURE_STORAGE_KEY>\"\n",
                ")\n",
                "print(\" Spark config ready.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "###  Path Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import functions as F\n",
                "from pyspark.sql.window import Window\n",
                "\n",
                "container_name  = \"annie-data\"\n",
                "storage_account = \"anniedatalake123\"\n",
                "base_path       = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/\"\n",
                "silver_path     = base_path + \"silver/\"\n",
                "gold_path       = base_path + \"gold/\"\n",
                "\n",
                "print(f\"Silver : {silver_path}\")\n",
                "print(f\"Gold   : {gold_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "###  Register Silver Tables + Schema Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Silver schemas after normalize_columns() in 02_silver_cleaning:\n",
                "#   sales      : store, brand, description, size, classification,\n",
                "#                sales_quantity, sales_dollars, sales_price, sales_date  <-- NOTE: sales_date (not sale_date)\n",
                "#   purchases  : brand, description, vendor_name, vendor_number, quantity,\n",
                "#                purchase_price, cost_per_unit, total_cost, po_date, receiving_date, invoice_date\n",
                "#   beg_inventory / end_inventory : brand, description, size, on_hand, price, inventory_id\n",
                "\n",
                "spark.read.format(\"delta\").load(silver_path + \"sales\").createOrReplaceTempView(\"sv_sales\")\n",
                "spark.read.format(\"delta\").load(silver_path + \"purchases\").createOrReplaceTempView(\"sv_purchases\")\n",
                "spark.read.format(\"delta\").load(silver_path + \"beg_inventory\").createOrReplaceTempView(\"sv_beg_inv\")\n",
                "spark.read.format(\"delta\").load(silver_path + \"end_inventory\").createOrReplaceTempView(\"sv_end_inv\")\n",
                "\n",
                "print(\" Silver views registered.\")\n",
                "print(\"sv_sales cols    :\", spark.table(\"sv_sales\").columns)\n",
                "print(\"sv_purchases cols:\", spark.table(\"sv_purchases\").columns)\n",
                "print(\"sv_beg_inv cols  :\", spark.table(\"sv_beg_inv\").columns)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "###  Helper Writer Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def write_gold(df, table_name):\n",
                "    \"\"\"\n",
                "    Saves a DataFrame as a MANAGED Delta table in hive_metastore.annie_gold.\n",
                "    Readable from Power BI via SQL Warehouse without any ADLS key config.\n",
                "    \"\"\"\n",
                "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS hive_metastore.annie_gold\")\n",
                "    (\n",
                "        df.write\n",
                "          .format(\"delta\")\n",
                "          .mode(\"overwrite\")\n",
                "          .option(\"overwriteSchema\", \"true\")\n",
                "          .saveAsTable(f\"hive_metastore.annie_gold.{table_name}\")\n",
                "    )\n",
                "    count = spark.table(f\"hive_metastore.annie_gold.{table_name}\").count()\n",
                "    print(f\"    gold.{table_name} → {count:,} rows | {len(df.columns)} cols\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "##  Intermediate Steps — Cost Lookup + Cached Enriched Sales\n",
                "\n",
                "These are **not** Gold tables — in-memory helpers that feed all 10 tables below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Median cost per (brand, description) from purchase invoices\n",
                "cost_lookup = spark.sql(\"\"\"\n",
                "    SELECT\n",
                "        brand,\n",
                "        description,\n",
                "        PERCENTILE_APPROX(cost_per_unit, 0.5) AS median_cost_per_unit\n",
                "    FROM sv_purchases\n",
                "    WHERE cost_per_unit IS NOT NULL AND cost_per_unit > 0\n",
                "    GROUP BY brand, description\n",
                "\"\"\")\n",
                "cost_lookup.createOrReplaceTempView(\"cost_lookup\")\n",
                "print(f\" cost_lookup: {cost_lookup.count():,} SKUs with known cost\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Row-level enriched sales with profit & margin per transaction.\n",
                "# Cached in memory — all 10 Gold tables aggregate from this view.\n",
                "# NOT saved to Gold as-is (would be millions of rows in Power BI).\n",
                "#\n",
                "# FIX: Silver 'sales' table uses 'sales_date' (SalesDate normalized by normalize_columns),\n",
                "#      NOT 'sale_date'. All date references below use s.sales_date.\n",
                "sales_enriched_raw = spark.sql(\"\"\"\n",
                "    SELECT\n",
                "        s.store,\n",
                "        s.brand,\n",
                "        s.description,\n",
                "        s.size,\n",
                "        s.classification,\n",
                "        s.sales_quantity,\n",
                "        s.sales_dollars,\n",
                "        s.sales_price,\n",
                "        s.sales_date,\n",
                "        COALESCE(c.median_cost_per_unit, s.sales_price * 0.60) AS cost_per_unit,\n",
                "        ROUND(\n",
                "            s.sales_dollars\n",
                "            - (COALESCE(c.median_cost_per_unit, s.sales_price * 0.60) * s.sales_quantity),\n",
                "            2\n",
                "        ) AS profit_dollars,\n",
                "        ROUND(\n",
                "            CASE\n",
                "                WHEN s.sales_dollars = 0 OR s.sales_dollars IS NULL THEN NULL\n",
                "                ELSE ((s.sales_dollars\n",
                "                       - COALESCE(c.median_cost_per_unit, s.sales_price * 0.60) * s.sales_quantity)\n",
                "                      / s.sales_dollars) * 100\n",
                "            END,\n",
                "            2\n",
                "        ) AS margin_pct\n",
                "    FROM sv_sales s\n",
                "    LEFT JOIN cost_lookup c\n",
                "        ON  CAST(s.brand AS STRING) = CAST(c.brand AS STRING)\n",
                "        AND s.description            = c.description\n",
                "\"\"\")\n",
                "\n",
                "sales_enriched_raw = sales_enriched_raw.repartition(8).cache()\n",
                "total_raw = sales_enriched_raw.count()  # trigger cache\n",
                "sales_enriched_raw.createOrReplaceTempView(\"gold_se_raw\")\n",
                "print(f\" gold_se_raw cached: {total_raw:,} rows (in-memory only, not saved)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "##  Gold Tables"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Table 1 — `gold.sales_enriched`\n",
                "**Grain:** (brand, description, size, classification, store)  \n",
                "**DAX measures:** Total Revenue, Total Profit, Total Units Sold, Total Volume"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sales_enriched_df = spark.sql(\"\"\"\n",
                "    SELECT\n",
                "        brand,\n",
                "        description,\n",
                "        size,\n",
                "        classification,\n",
                "        store,\n",
                "        SUM(sales_quantity)                      AS sales_quantity,\n",
                "        ROUND(SUM(sales_dollars), 2)             AS sales_dollars,\n",
                "        ROUND(SUM(profit_dollars), 2)            AS profit_dollars,\n",
                "        ROUND(\n",
                "            SUM(sales_quantity) * CASE\n",
                "                WHEN UPPER(size) LIKE '%1.75L%' OR UPPER(size) LIKE '%1750%' THEN 1.75\n",
                "                WHEN UPPER(size) LIKE '%1L%'   OR UPPER(size) LIKE '%1000%' THEN 1.00\n",
                "                WHEN UPPER(size) LIKE '%750%'                                THEN 0.75\n",
                "                WHEN UPPER(size) LIKE '%500%'                                THEN 0.50\n",
                "                WHEN UPPER(size) LIKE '%375%'                                THEN 0.375\n",
                "                WHEN UPPER(size) LIKE '%200%'                                THEN 0.20\n",
                "                WHEN UPPER(size) LIKE '%100%'                                THEN 0.10\n",
                "                ELSE 0.75\n",
                "            END,\n",
                "        2) AS volume\n",
                "    FROM gold_se_raw\n",
                "    GROUP BY brand, description, size, classification, store\n",
                "\"\"\")\n",
                "\n",
                "sales_enriched_df = sales_enriched_df.repartition(4).cache()\n",
                "sales_enriched_df.count()\n",
                "write_gold(sales_enriched_df, \"sales_enriched\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Table 2 — `gold.product_profitability`\n",
                "**Grain:** (brand, description, size, classification)  \n",
                "**Visuals:** Top 10 Products by Profit, Top 10 by Margin, Scatter Plot, Brand slicer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "product_profitability = spark.sql(\"\"\"\n",
                "    WITH base AS (\n",
                "        SELECT\n",
                "            brand,\n",
                "            description,\n",
                "            size,\n",
                "            classification,\n",
                "            SUM(sales_quantity)              AS total_units_sold,\n",
                "            ROUND(SUM(sales_dollars), 2)     AS total_revenue,\n",
                "            ROUND(SUM(profit_dollars), 2)    AS total_profit_dollars,\n",
                "            ROUND(AVG(margin_pct), 2)        AS avg_margin_pct\n",
                "        FROM gold_se_raw\n",
                "        GROUP BY brand, description, size, classification\n",
                "    )\n",
                "    SELECT\n",
                "        brand, description, size, classification,\n",
                "        total_units_sold, total_revenue, total_profit_dollars, avg_margin_pct,\n",
                "        CASE WHEN total_profit_dollars < 0 THEN true ELSE false END AS is_loss_maker,\n",
                "        ROW_NUMBER() OVER (ORDER BY total_profit_dollars DESC)      AS rank_by_profit,\n",
                "        ROW_NUMBER() OVER (ORDER BY avg_margin_pct DESC)            AS rank_by_margin\n",
                "    FROM base\n",
                "    WHERE total_units_sold > 0\n",
                "\"\"\")\n",
                "\n",
                "product_profitability = product_profitability.repartition(4).cache()\n",
                "product_profitability.count()\n",
                "write_gold(product_profitability, \"product_profitability\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Table 3 — `gold.brand_profitability`\n",
                "**Grain:** brand  \n",
                "**Visuals:** Top 10 Brands by Profit, Top 10 by Margin, Revenue vs Profit clustered bar"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "brand_profitability = spark.sql(\"\"\"\n",
                "    WITH base AS (\n",
                "        SELECT\n",
                "            brand,\n",
                "            FIRST(classification)            AS classification,\n",
                "            COUNT(DISTINCT description)      AS sku_count,\n",
                "            SUM(sales_quantity)              AS total_units_sold,\n",
                "            ROUND(SUM(sales_dollars), 2)     AS total_revenue,\n",
                "            ROUND(SUM(profit_dollars), 2)    AS total_profit_dollars,\n",
                "            ROUND(AVG(margin_pct), 2)        AS avg_margin_pct\n",
                "        FROM gold_se_raw\n",
                "        GROUP BY brand\n",
                "    )\n",
                "    SELECT\n",
                "        brand, classification, sku_count,\n",
                "        total_units_sold, total_revenue, total_profit_dollars, avg_margin_pct,\n",
                "        CASE WHEN total_profit_dollars < 0 THEN true ELSE false END AS is_loss_maker,\n",
                "        ROW_NUMBER() OVER (ORDER BY total_profit_dollars DESC)      AS rank_by_profit,\n",
                "        ROW_NUMBER() OVER (ORDER BY avg_margin_pct DESC)            AS rank_by_margin\n",
                "    FROM base\n",
                "    WHERE total_units_sold > 0\n",
                "\"\"\")\n",
                "\n",
                "brand_profitability = brand_profitability.repartition(4).cache()\n",
                "brand_profitability.count()\n",
                "write_gold(brand_profitability, \"brand_profitability\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Table 4 — `gold.loss_makers`\n",
                "**Grain:** (brand, description, size) where SUM(profit) < 0  \n",
                "**Visual:** Loss-Making Items Table (Page 1) — all columns shown in table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "loss_makers = spark.sql(\"\"\"\n",
                "    WITH sku_agg AS (\n",
                "        SELECT\n",
                "            'PRODUCT'                            AS level,\n",
                "            brand,\n",
                "            description,\n",
                "            size,\n",
                "            FIRST(classification)                AS classification,\n",
                "            COUNT(DISTINCT store)                AS stores_stocking,\n",
                "            SUM(sales_quantity)                  AS total_units_sold,\n",
                "            ROUND(SUM(sales_dollars), 2)         AS total_revenue,\n",
                "            ROUND(SUM(profit_dollars), 2)        AS total_profit_dollars,\n",
                "            ROUND(AVG(margin_pct), 2)            AS avg_margin_pct\n",
                "        FROM gold_se_raw\n",
                "        GROUP BY brand, description, size\n",
                "        HAVING SUM(profit_dollars) < 0\n",
                "    )\n",
                "    SELECT\n",
                "        level, brand, description, size, classification,\n",
                "        stores_stocking, total_units_sold, total_revenue,\n",
                "        total_profit_dollars, avg_margin_pct,\n",
                "        CASE\n",
                "            WHEN avg_margin_pct < -10 THEN 'DROP — Severe loss, consider discontinuing'\n",
                "            WHEN avg_margin_pct <   0 THEN 'REVIEW — Renegotiate cost or reprice'\n",
                "            ELSE                           'MONITOR — Minor loss, watch trend'\n",
                "        END AS recommendation\n",
                "    FROM sku_agg\n",
                "    ORDER BY total_profit_dollars ASC\n",
                "\"\"\")\n",
                "\n",
                "loss_makers = loss_makers.repartition(4).cache()\n",
                "loss_makers.count()\n",
                "write_gold(loss_makers, \"loss_makers\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Table 5 — `gold.sales_by_store`\n",
                "**Grain:** store  \n",
                "**Visual:** Map (bubble size=revenue, color=margin), Store slicer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sales_by_store = spark.sql(\"\"\"\n",
                "    SELECT\n",
                "        store,\n",
                "        COUNT(*)                             AS transaction_count,\n",
                "        COUNT(DISTINCT brand)                AS unique_brands,\n",
                "        COUNT(DISTINCT description)          AS unique_skus,\n",
                "        SUM(sales_quantity)                  AS total_units_sold,\n",
                "        ROUND(SUM(sales_dollars), 2)         AS total_revenue,\n",
                "        ROUND(SUM(profit_dollars), 2)        AS total_profit_dollars,\n",
                "        ROUND(AVG(margin_pct), 2)            AS avg_margin_pct\n",
                "    FROM gold_se_raw\n",
                "    GROUP BY store\n",
                "    ORDER BY total_revenue DESC\n",
                "\"\"\")\n",
                "\n",
                "sales_by_store = sales_by_store.repartition(4).cache()\n",
                "sales_by_store.count()\n",
                "write_gold(sales_by_store, \"sales_by_store\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Table 6 — `gold.sales_time_series`\n",
                "**Grain:** year + month (max ~12 rows)  \n",
                "**Visual:** Monthly Trend combo chart (bars=revenue, line=profit, dashed=margin)\n",
                "\n",
                "> **FIX:** Silver `sales` table has column `sales_date` (from `SalesDate` CSV header after normalize_columns).  \n",
                "> All date functions below reference `sales_date` — aliased to `sale_year`/`sale_month` for Power BI."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sales_time_series = spark.sql(\"\"\"\n",
                "    SELECT\n",
                "        YEAR(sales_date)                     AS sale_year,\n",
                "        MONTH(sales_date)                    AS sale_month,\n",
                "        DATE_FORMAT(sales_date, 'MMMM')      AS sale_month_name,\n",
                "        ROUND(SUM(sales_dollars), 2)         AS monthly_revenue,\n",
                "        ROUND(SUM(profit_dollars), 2)        AS monthly_profit,\n",
                "        ROUND(AVG(margin_pct), 2)            AS avg_margin_pct,\n",
                "        SUM(sales_quantity)                  AS total_units_sold,\n",
                "        COUNT(DISTINCT brand)                AS active_brands,\n",
                "        COUNT(DISTINCT store)                AS active_stores,\n",
                "        COUNT(*)                             AS transaction_count\n",
                "    FROM gold_se_raw\n",
                "    WHERE sales_date IS NOT NULL\n",
                "    GROUP BY YEAR(sales_date), MONTH(sales_date), DATE_FORMAT(sales_date, 'MMMM')\n",
                "    ORDER BY sale_year, sale_month\n",
                "\"\"\")\n",
                "\n",
                "sales_time_series = sales_time_series.repartition(4).cache()\n",
                "sales_time_series.count()\n",
                "write_gold(sales_time_series, \"sales_time_series\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Table 7 — `gold.inventory_delta`\n",
                "**Grain:** (brand, description, size) present in beg or end inventory  \n",
                "**Visual:** Inventory Health Table (all columns displayed)\n",
                "\n",
                "> Silver `beg_inventory`/`end_inventory` column is `price` (from `Price` CSV → normalize_columns)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inventory_delta = spark.sql(\"\"\"\n",
                "    WITH beg AS (\n",
                "        SELECT brand, description, size,\n",
                "               SUM(on_hand) AS beg_on_hand, AVG(price) AS beg_avg_cost\n",
                "        FROM sv_beg_inv WHERE on_hand IS NOT NULL\n",
                "        GROUP BY brand, description, size\n",
                "    ),\n",
                "    ends AS (\n",
                "        SELECT brand, description, size,\n",
                "               SUM(on_hand) AS end_on_hand, AVG(price) AS end_avg_cost\n",
                "        FROM sv_end_inv WHERE on_hand IS NOT NULL\n",
                "        GROUP BY brand, description, size\n",
                "    ),\n",
                "    vendor_map AS (\n",
                "        SELECT brand, description, vendor_name,\n",
                "               ROW_NUMBER() OVER (PARTITION BY brand, description ORDER BY brand) AS rn\n",
                "        FROM sv_purchases WHERE vendor_name IS NOT NULL\n",
                "    )\n",
                "    SELECT\n",
                "        COALESCE(b.brand, e.brand)              AS brand,\n",
                "        COALESCE(b.description, e.description)  AS description,\n",
                "        COALESCE(b.size, e.size)                AS size,\n",
                "        COALESCE(vm.vendor_name, 'Unknown')     AS vendor_name,\n",
                "        COALESCE(b.beg_on_hand, 0)              AS beg_on_hand,\n",
                "        COALESCE(e.end_on_hand, 0)              AS end_on_hand,\n",
                "        (COALESCE(e.end_on_hand, 0)\n",
                "         - COALESCE(b.beg_on_hand, 0))          AS inventory_change,\n",
                "        ROUND(\n",
                "            (COALESCE(e.end_on_hand, 0) * COALESCE(e.end_avg_cost, 0))\n",
                "          - (COALESCE(b.beg_on_hand, 0) * COALESCE(b.beg_avg_cost, 0)),\n",
                "        2)                                      AS inventory_value_change,\n",
                "        CASE\n",
                "            WHEN COALESCE(e.end_on_hand, 0) = 0\n",
                "             AND COALESCE(b.beg_on_hand, 0) > 0  THEN 'DEPLETED'\n",
                "            WHEN COALESCE(e.end_on_hand, 0)\n",
                "               > COALESCE(b.beg_on_hand, 0) * 1.2  THEN 'OVERSTOCKED'\n",
                "            ELSE 'STABLE'\n",
                "        END AS stock_status\n",
                "    FROM beg b\n",
                "    FULL OUTER JOIN ends e\n",
                "        ON  CAST(b.brand AS STRING) = CAST(e.brand AS STRING)\n",
                "        AND b.description           = e.description\n",
                "        AND b.size                  = e.size\n",
                "    LEFT JOIN vendor_map vm\n",
                "        ON  CAST(COALESCE(b.brand, e.brand) AS STRING) = CAST(vm.brand AS STRING)\n",
                "        AND COALESCE(b.description, e.description)     = vm.description\n",
                "        AND vm.rn = 1\n",
                "    WHERE COALESCE(b.beg_on_hand, 0) > 0\n",
                "       OR COALESCE(e.end_on_hand, 0) > 0\n",
                "\"\"\")\n",
                "\n",
                "inventory_delta = inventory_delta.repartition(4).cache()\n",
                "inventory_delta.count()\n",
                "write_gold(inventory_delta, \"inventory_delta\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Table 8 — `gold.vendor_performance`\n",
                "**Grain:** vendor_name  \n",
                "**Visual:** Top 10 Vendors bar chart, Vendor slicer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vendor_performance = spark.sql(\"\"\"\n",
                "    SELECT\n",
                "        vendor_name,\n",
                "        COUNT(DISTINCT brand)                    AS brands_supplied,\n",
                "        COUNT(DISTINCT description)              AS skus_supplied,\n",
                "        ROUND(SUM(cost_per_unit * quantity), 2)  AS total_purchase_spend,\n",
                "        ROUND(AVG(cost_per_unit), 2)             AS avg_cost_per_unit,\n",
                "        ROUND(AVG(\n",
                "            CASE\n",
                "                WHEN receiving_date IS NOT NULL AND po_date IS NOT NULL\n",
                "                THEN DATEDIFF(receiving_date, po_date)\n",
                "                ELSE NULL\n",
                "            END\n",
                "        ), 1)                                    AS avg_lead_time_days,\n",
                "        COUNT(*)                                 AS total_po_count\n",
                "    FROM sv_purchases\n",
                "    WHERE vendor_name IS NOT NULL\n",
                "    GROUP BY vendor_name\n",
                "    ORDER BY total_purchase_spend DESC\n",
                "\"\"\")\n",
                "\n",
                "vendor_performance = vendor_performance.repartition(4).cache()\n",
                "vendor_performance.count()\n",
                "write_gold(vendor_performance, \"vendor_performance\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Table 9 — `gold.size_analysis`\n",
                "**Grain:** size  \n",
                "**Visual:** Profit by Bottle Size column chart, Size slicer (Page 1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "size_analysis = spark.sql(\"\"\"\n",
                "    SELECT\n",
                "        size,\n",
                "        COUNT(DISTINCT description)          AS unique_skus,\n",
                "        COUNT(DISTINCT brand)                AS unique_brands,\n",
                "        SUM(sales_quantity)                  AS total_units_sold,\n",
                "        ROUND(SUM(profit_dollars), 2)        AS total_profit_dollars,\n",
                "        ROUND(AVG(margin_pct), 2)            AS avg_margin_pct,\n",
                "        ROUND(AVG(sales_price), 2)           AS avg_selling_price\n",
                "    FROM gold_se_raw\n",
                "    WHERE size IS NOT NULL\n",
                "    GROUP BY size\n",
                "    ORDER BY total_profit_dollars DESC\n",
                "\"\"\")\n",
                "\n",
                "size_analysis = size_analysis.repartition(4).cache()\n",
                "size_analysis.count()\n",
                "write_gold(size_analysis, \"size_analysis\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Table 10 — `gold.classification_performance`\n",
                "**Grain:** classification  \n",
                "**Visual:** Treemap (size=revenue, color=margin), Classification slicer (Page 1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "classification_performance = spark.sql(\"\"\"\n",
                "    SELECT\n",
                "        classification,\n",
                "        COUNT(DISTINCT brand)                AS unique_brands,\n",
                "        COUNT(DISTINCT description)          AS unique_skus,\n",
                "        SUM(sales_quantity)                  AS total_units_sold,\n",
                "        ROUND(SUM(sales_dollars), 2)         AS total_revenue,\n",
                "        ROUND(SUM(profit_dollars), 2)        AS total_profit_dollars,\n",
                "        ROUND(AVG(margin_pct), 2)            AS overall_margin_pct\n",
                "    FROM gold_se_raw\n",
                "    WHERE classification IS NOT NULL\n",
                "    GROUP BY classification\n",
                "    ORDER BY total_revenue DESC\n",
                "\"\"\")\n",
                "\n",
                "classification_performance = classification_performance.repartition(4).cache()\n",
                "classification_performance.count()\n",
                "write_gold(classification_performance, \"classification_performance\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "##  Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gold_tables = [\n",
                "    \"sales_enriched\",\n",
                "    \"product_profitability\",\n",
                "    \"brand_profitability\",\n",
                "    \"loss_makers\",\n",
                "    \"sales_by_store\",\n",
                "    \"sales_time_series\",\n",
                "    \"inventory_delta\",\n",
                "    \"vendor_performance\",\n",
                "    \"size_analysis\",\n",
                "    \"classification_performance\"\n",
                "]\n",
                "\n",
                "print(\"=\" * 65)\n",
                "print(\" GOLD LAYER — FINAL SUMMARY\")\n",
                "print(\"=\" * 65)\n",
                "total_rows = 0\n",
                "for t in gold_tables:\n",
                "    try:\n",
                "        df = spark.table(f\"hive_metastore.annie_gold.{t}\")\n",
                "        n  = df.count()\n",
                "        total_rows += n\n",
                "        print(f\"     {t:<35} → {n:>8,} rows | {len(df.columns)} cols\")\n",
                "    except Exception as ex:\n",
                "        print(f\"     {t:<35} → ERROR: {ex}\")\n",
                "\n",
                "print(\"-\" * 65)\n",
                "print(f\"   Total rows across all Gold tables : {total_rows:,}\")\n",
                "print(\"=\" * 65)\n",
                "print(\" Connect via: Databricks → Partner Connect → Power BI → SQL Warehouse\")\n",
                "print(\"   Schema: hive_metastore.annie_gold\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}